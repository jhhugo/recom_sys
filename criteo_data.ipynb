{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13 integer features and 26 categorical features\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# Clip integer features. The clip point for each integer feature\n",
    "# is derived from the 95% quantile of the total values in each feature\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Normalize the integer features to [0, 1] by min-max normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "        self.min = [sys.maxsize] * num_feature\n",
    "        self.max = [-sys.maxsize] * num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    val = features[continous_features[i]]\n",
    "                    if val != '':\n",
    "                        val = int(val)\n",
    "                        if val > continous_clip[i]:\n",
    "                            val = continous_clip[i]\n",
    "                        self.min[i] = min(self.min[i], val)\n",
    "                        self.max[i] = max(self.max[i], val)\n",
    "\n",
    "    def gen(self, idx, val):\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        val = float(val)\n",
    "        return (val - self.min[idx]) / (self.max[idx] - self.min[idx])\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    Generate dictionary for each of the categorical features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_feature):\n",
    "        self.dicts = []\n",
    "        self.num_feature = num_feature\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "                for i in range(0, self.num_feature):\n",
    "                    if features[categorial_features[i]] != '':\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "        for i in range(0, self.num_feature):\n",
    "            # 去点频率小于cutoff的特征\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            # 每个字符值编码\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        if key not in self.dicts[idx]:\n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        return res\n",
    "\n",
    "    def dicts_sizes(self):\n",
    "        return list(map(len, self.dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/criteo\"\n",
    "outdir = \"../data/criteo/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \"\"\"\n",
    "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "    dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    dicts = CategoryDictGenerator(len(categorial_features))\n",
    "    dicts.build(\n",
    "        os.path.join(datadir, 'train.txt'), categorial_features, cutoff=10)#200 50\n",
    "\n",
    "    dict_sizes = dicts.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "                for line in tqdm_notebook(f):\n",
    "                    features = line.rstrip('\\n').split('\\t')\n",
    "                    continous_feats = []\n",
    "                    continous_vals = []\n",
    "                    for i in range(0, len(continous_features)):\n",
    "\n",
    "                        val = dists.gen(i, features[continous_features[i]])\n",
    "                        continous_vals.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                        continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                    categorial_lgb_vals = []\n",
    "                    for i in range(0, len(categorial_features)):\n",
    "                        val_lgb = dicts.gen(i, features[categorial_features[i]])\n",
    "                        categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                    continous_vals = ','.join(continous_vals)\n",
    "                    categorial_lgb_vals = ','.join(categorial_lgb_vals)\n",
    "                    label = features[0]\n",
    "                    if random.randint(0, 9999) % 10 != 0:               \n",
    "                        out_train.write(','.join([continous_vals, categorial_lgb_vals, label]) + '\\n')\n",
    "                        \n",
    "                    else:\n",
    "                        out_valid.write(','.join([continous_vals, categorial_lgb_vals, label]) + '\\n')\n",
    "                        \n",
    "\n",
    "\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "            for line in tqdm_notebook(f):\n",
    "                features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "                continous_feats = []\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = dists.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                    continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                categorial_lgb_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "\n",
    "                    val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
    "                    categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_lgb_vals = ','.join(categorial_lgb_vals)\n",
    "                \n",
    "                out.write(','.join([continous_vals, categorial_lgb_vals]) + '\\n')\n",
    "\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \"\"\"\n",
    "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "    dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "    dicts = CategoryDictGenerator(len(categorial_features))\n",
    "    dicts.build(\n",
    "        os.path.join(datadir, 'train.txt'), categorial_features, cutoff=10)#200 50\n",
    "\n",
    "    dict_sizes = dicts.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "    train_fm = open(os.path.join(outdir, 'train_fm.txt'), 'wb')\n",
    "    valid_fm = open(os.path.join(outdir, 'valid_fm.txt'), 'wb')\n",
    "\n",
    "#     train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "#     valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "#     with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "#         with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "    with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "        for line in tqdm_notebook(f):\n",
    "            features = line.rstrip('\\n').split('\\t')\n",
    "            continous_feats = []\n",
    "            continous_vals = []\n",
    "            for i in range(0, len(continous_features)):\n",
    "\n",
    "                val = dists.gen(i, features[continous_features[i]])\n",
    "                continous_vals.append(\n",
    "                    \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                continous_feats.append(\n",
    "                    \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "            categorial_vals = []\n",
    "            categorial_lgb_vals = []\n",
    "            for i in range(0, len(categorial_features)):\n",
    "                val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "                categorial_vals.append(str(val))\n",
    "\n",
    "            continous_vals = ','.join(continous_vals)\n",
    "            categorial_vals = ','.join(categorial_vals)\n",
    "            label = features[0]\n",
    "            if random.randint(0, 9999) % 10 != 0:\n",
    "#                         out_train.write(','.join(\n",
    "#                             [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                train_val = []\n",
    "                train_val.append(str(label))\n",
    "                train_val.extend(['{}:{}'.format(ii, val) for ii,val in enumerate(continous_vals.split(','))])\n",
    "                train_val.extend(['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')])\n",
    "                train_fm.write((\" \".join(train_val) + \"\\n\").encode('ascii'))\n",
    "#                 train_fm.write('\\t'.join(label) + '\\t')\n",
    "#                 train_fm.write(('\\t'.join(\n",
    "#                     ['{}:{}'.format(ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')).encode('ascii')\n",
    "#                 train_fm.write(('\\t'.join(\n",
    "#                     ['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')]) + '\\n')).encode('ascii')\n",
    "\n",
    "\n",
    "            else:\n",
    "#                         out_valid.write(','.join(\n",
    "#                             [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                valid_val = []\n",
    "                valid_val.append(str(label))\n",
    "                valid_val.extend(['{}:{}'.format(ii, val) for ii,val in enumerate(continous_vals.split(','))])\n",
    "                valid_val.extend(['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')])\n",
    "                valid_fm.write((\" \".join(valid_val) + \"\\n\").encode('ascii'))\n",
    "#                 valid_fm.write('\\t'.join(label) + '\\t')\n",
    "#                 valid_fm.write(('\\t'.join(\n",
    "#                     ['{}:{}'.format(ii, val) for ii, val in enumerate(continous_vals.split(','))]) + '\\t')).encode('ascii')\n",
    "#                 valid_fm.write(('\\t'.join(\n",
    "#                     ['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')]) + '\\n')).encode('ascii')\n",
    "                                            \n",
    "                        \n",
    "    train_fm.close()\n",
    "    valid_fm.close()\n",
    "\n",
    "\n",
    "    test_fm = open(os.path.join(outdir, 'test_fm.txt'), 'wb')\n",
    "\n",
    "#     with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "    with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "        for line in tqdm_notebook(f):\n",
    "            features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "            continous_feats = []\n",
    "            continous_vals = []\n",
    "            for i in range(0, len(continous_features)):\n",
    "                val = dists.gen(i, features[continous_features[i] - 1])\n",
    "                continous_vals.append(\n",
    "                    \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                continous_feats.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "            categorial_vals = []\n",
    "            categorial_lgb_vals = []\n",
    "            for i in range(0, len(categorial_features)):\n",
    "                val = dicts.gen(i,\n",
    "                                features[categorial_features[i] -\n",
    "                                         1]) + categorial_feature_offset[i]\n",
    "                categorial_vals.append(str(val))\n",
    "\n",
    "                val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
    "                categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "            continous_vals = ','.join(continous_vals)\n",
    "            categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "#                 out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "            test_val = []\n",
    "            test_val.extend(['{}:{}'.format(ii, val) for ii,val in enumerate(continous_vals.split(','))])\n",
    "            test_val.extend(['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')])\n",
    "            test_fm.write((\" \".join(test_val) + \"\\n\").encode('ascii'))\n",
    "#             test_fm.write(('\\t'.join(['{}:{}'.format(ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t').encode('ascii'))\n",
    "#             test_fm.write(('\\t'.join(\n",
    "#                 ['{}:1'.format(str(np.int32(val) + 13)) for val in categorial_vals.split(',')]) + '\\n').encode('ascii'))\n",
    "                                                                \n",
    "    test_fm.close()\n",
    "    return dict_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(datadir, outdir):\n",
    "#     \"\"\"\n",
    "#     All the 13 integer features are normalzied to continous values and these\n",
    "#     continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "#     Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "#     vectors are combined into one sparse binary vector.\n",
    "#     \"\"\"\n",
    "#     dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "#     dists.build(os.path.join(datadir, 'train.txt'), continous_features)\n",
    "\n",
    "#     dicts = CategoryDictGenerator(len(categorial_features))\n",
    "#     dicts.build(\n",
    "#         os.path.join(datadir, 'train.txt'), categorial_features, cutoff=200)#200 50\n",
    "\n",
    "#     dict_sizes = dicts.dicts_sizes()\n",
    "#     categorial_feature_offset = [0]\n",
    "#     for i in range(1, len(categorial_features)):\n",
    "#         offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
    "#         categorial_feature_offset.append(offset)\n",
    "\n",
    "#     random.seed(0)\n",
    "\n",
    "#     # 90% of the data are used for training, and 10% of the data are used\n",
    "#     # for validation.\n",
    "#     train_ffm = open(os.path.join(outdir, 'train_ffm.txt'), 'w')\n",
    "#     valid_ffm = open(os.path.join(outdir, 'valid_ffm.txt'), 'w')\n",
    "\n",
    "# #     train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "# #     valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "#     with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "#         with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "#             with open(os.path.join(datadir, 'train.txt'), 'r') as f:\n",
    "#                 for line in tqdm_notebook(f):\n",
    "#                     features = line.rstrip('\\n').split('\\t')\n",
    "#                     continous_feats = []\n",
    "#                     continous_vals = []\n",
    "#                     for i in range(0, len(continous_features)):\n",
    "\n",
    "#                         val = dists.gen(i, features[continous_features[i]])\n",
    "#                         continous_vals.append(\n",
    "#                             \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "#                         continous_feats.append(\n",
    "#                             \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "#                     categorial_vals = []\n",
    "#                     categorial_lgb_vals = []\n",
    "#                     for i in range(0, len(categorial_features)):\n",
    "#                         val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "#                         categorial_vals.append(str(val))\n",
    "#                         val_lgb = dicts.gen(i, features[categorial_features[i]])\n",
    "#                         categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "#                     continous_vals = ','.join(continous_vals)\n",
    "#                     categorial_vals = ','.join(categorial_vals)\n",
    "#                     label = features[0]\n",
    "#                     if random.randint(0, 9999) % 10 != 0:\n",
    "#                         out_train.write(','.join(\n",
    "#                             [continous_vals, categorial_vals, label]) + '\\n')\n",
    "#                         train_ffm.write('\\t'.join(label) + '\\t')\n",
    "#                         train_ffm.write('\\t'.join(\n",
    "#                             ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "#                         train_ffm.write('\\t'.join(\n",
    "#                             ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                        \n",
    "# #                         train_lgb.write('\\t'.join(label) + '\\t')\n",
    "# #                         train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "# #                         train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "#                     else:\n",
    "#                         out_valid.write(','.join(\n",
    "#                             [continous_vals, categorial_vals, label]) + '\\n')\n",
    "#                         valid_ffm.write('\\t'.join(label) + '\\t')\n",
    "#                         valid_ffm.write('\\t'.join(\n",
    "#                             ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "#                         valid_ffm.write('\\t'.join(\n",
    "#                             ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                \n",
    "# #                         valid_lgb.write('\\t'.join(label) + '\\t')\n",
    "# #                         valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "# #                         valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "                        \n",
    "#     train_ffm.close()\n",
    "#     valid_ffm.close()\n",
    "\n",
    "# #     train_lgb.close()\n",
    "# #     valid_lgb.close()\n",
    "\n",
    "#     test_ffm = open(os.path.join(outdir, 'test_ffm.txt'), 'w')\n",
    "# #     test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
    "\n",
    "#     with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "#         with open(os.path.join(datadir, 'test.txt'), 'r') as f:\n",
    "#             for line in tqdm_notebook(f):\n",
    "#                 features = line.rstrip('\\n').split('\\t')\n",
    "\n",
    "#                 continous_feats = []\n",
    "#                 continous_vals = []\n",
    "#                 for i in range(0, len(continous_features)):\n",
    "#                     val = dists.gen(i, features[continous_features[i] - 1])\n",
    "#                     continous_vals.append(\n",
    "#                         \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "#                     continous_feats.append(\n",
    "#                             \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "#                 categorial_vals = []\n",
    "#                 categorial_lgb_vals = []\n",
    "#                 for i in range(0, len(categorial_features)):\n",
    "#                     val = dicts.gen(i,\n",
    "#                                     features[categorial_features[i] -\n",
    "#                                              1]) + categorial_feature_offset[i]\n",
    "#                     categorial_vals.append(str(val))\n",
    "\n",
    "#                     val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
    "#                     categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "#                 continous_vals = ','.join(continous_vals)\n",
    "#                 categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "#                 out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "                \n",
    "#                 test_ffm.write('\\t'.join(['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "#                 test_ffm.write('\\t'.join(\n",
    "#                     ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                                \n",
    "# #                 test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "# #                 test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "#     test_ffm.close()\n",
    "# #     test_lgb.close()\n",
    "#     return dict_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf9c8063e1d462aa06c6efb15918337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f463f9b1e77545888726048108874bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[532,\n",
       " 533,\n",
       " 11289,\n",
       " 13822,\n",
       " 151,\n",
       " 13,\n",
       " 9005,\n",
       " 254,\n",
       " 4,\n",
       " 11557,\n",
       " 4191,\n",
       " 11560,\n",
       " 3037,\n",
       " 27,\n",
       " 5640,\n",
       " 12459,\n",
       " 11,\n",
       " 2790,\n",
       " 1391,\n",
       " 4,\n",
       " 12005,\n",
       " 10,\n",
       " 15,\n",
       " 9617,\n",
       " 52,\n",
       " 7599]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(datadir=\"../data/criteo\", outdir=\"../data/criteo/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97b072c1757461f90f985faa16c16f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35419b7cfb73489e8daf624ccc528607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[532,\n",
       " 533,\n",
       " 11289,\n",
       " 13822,\n",
       " 151,\n",
       " 13,\n",
       " 9005,\n",
       " 254,\n",
       " 4,\n",
       " 11557,\n",
       " 4191,\n",
       " 11560,\n",
       " 3037,\n",
       " 27,\n",
       " 5640,\n",
       " 12459,\n",
       " 11,\n",
       " 2790,\n",
       " 1391,\n",
       " 4,\n",
       " 12005,\n",
       " 10,\n",
       " 15,\n",
       " 9617,\n",
       " 52,\n",
       " 7599]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(datadir=datadir, outdir=outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc47523fb8b458f8d291667598da2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffefdce21f1044c0aa0a29efc0b15c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1458,\n",
       " 555,\n",
       " 193948,\n",
       " 138800,\n",
       " 306,\n",
       " 18,\n",
       " 11970,\n",
       " 634,\n",
       " 4,\n",
       " 42646,\n",
       " 5178,\n",
       " 192772,\n",
       " 3175,\n",
       " 27,\n",
       " 11422,\n",
       " 181074,\n",
       " 11,\n",
       " 4654,\n",
       " 2031,\n",
       " 4,\n",
       " 189656,\n",
       " 17,\n",
       " 16,\n",
       " 59696,\n",
       " 85,\n",
       " 45570]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(datadir=datadir, outdir=outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2c87d6d256434ca747a32eafc96546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1244fd52404d0c98379c0b67361993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dim_dict = preprocess(datadir=datadir, outdir=outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1458,\n",
       " 555,\n",
       " 193948,\n",
       " 138800,\n",
       " 306,\n",
       " 18,\n",
       " 11970,\n",
       " 634,\n",
       " 4,\n",
       " 42646,\n",
       " 5178,\n",
       " 192772,\n",
       " 3175,\n",
       " 27,\n",
       " 11422,\n",
       " 181074,\n",
       " 11,\n",
       " 4654,\n",
       " 2031,\n",
       " 4,\n",
       " 189656,\n",
       " 17,\n",
       " 16,\n",
       " 59696,\n",
       " 85,\n",
       " 45570]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_dict.dicts_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_dict = [1458,\n",
    " 555,\n",
    " 193948,\n",
    " 138800,\n",
    " 306,\n",
    " 18,\n",
    " 11970,\n",
    " 634,\n",
    " 4,\n",
    " 42646,\n",
    " 5178,\n",
    " 192772,\n",
    " 3175,\n",
    " 27,\n",
    " 11422,\n",
    " 181074,\n",
    " 11,\n",
    " 4654,\n",
    " 2031,\n",
    " 4,\n",
    " 189656,\n",
    " 17,\n",
    " 16,\n",
    " 59696,\n",
    " 85,\n",
    " 45570]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dim_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write((line_pattern % feat).encode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../data/criteo/output_fm\", \"train_sub1k_transform.txt\"), \"wb\") as out:\n",
    "    with open(os.path.join(\"../data/criteo/output_fm\", \"train_sub1k.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for v in f:\n",
    "            res = []\n",
    "            for i, pair in enumerate(v.split(\"\\t\")):\n",
    "                if i != 0:\n",
    "                    vals = pair.split(\":\")\n",
    "                    res.append(\"%d:%.16f\" % (int(vals[0]), float(vals[1])))\n",
    "                else:\n",
    "                    val = int(pair)\n",
    "                    res.append(\"%d\" % val)\n",
    "            out.write((\" \".join(res) + \"\\n\").encode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../data/criteo/output_fm\", \"test_sub1k_transform.txt\"), \"wb\") as out:\n",
    "    with open(os.path.join(\"../data/criteo/output_fm\", \"test_sub1k.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for v in f:\n",
    "            res = []\n",
    "            for i, pair in enumerate(v.split(\"\\t\")):\n",
    "                if i != 0:\n",
    "                    vals = pair.split(\":\")\n",
    "                    res.append(\"%d:%.16f\" % (int(vals[0]), float(vals[1])))\n",
    "                else:\n",
    "                    val = 0\n",
    "                    res.append(\"%d\" % val)\n",
    "            out.write((\" \".join(res) + \"\\n\").encode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"../data/criteo/output_fm\", \"train_sub1k.txt\"), \"r\") as f:\n",
    "    v = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\t0:0.05\\t1:0.006633\\t2:0.05\\t3:0\\t4:0.021594\\t5:0.008\\t6:0.15\\t7:0.04\\t8:0.362\\t9:0.1\\t10:0.2\\t11:0\\t12:0.04\\t15:1\\t555:1\\t1078:1\\t17797:1\\t26190:1\\t26341:1\\t28570:1\\t35361:1\\t35613:1\\t35984:1\\t48424:1\\t51364:1\\t64053:1\\t65964:1\\t66206:1\\t71628:1\\t84088:1\\t84119:1\\t86889:1\\t88280:1\\t88283:1\\t100288:1\\t100300:1\\t102447:1\\t109932:1\\t111823:1\\n'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0:0.05',\n",
       " '1:0.006633',\n",
       " '2:0.05',\n",
       " '3:0',\n",
       " '4:0.021594',\n",
       " '5:0.008',\n",
       " '6:0.15',\n",
       " '7:0.04',\n",
       " '8:0.362',\n",
       " '9:0.1',\n",
       " '10:0.2',\n",
       " '11:0',\n",
       " '12:0.04',\n",
       " '15:1',\n",
       " '555:1',\n",
       " '1078:1',\n",
       " '17797:1',\n",
       " '26190:1',\n",
       " '26341:1',\n",
       " '28570:1',\n",
       " '35361:1',\n",
       " '35613:1',\n",
       " '35984:1',\n",
       " '48424:1',\n",
       " '51364:1',\n",
       " '64053:1',\n",
       " '65964:1',\n",
       " '66206:1',\n",
       " '71628:1',\n",
       " '84088:1',\n",
       " '84119:1',\n",
       " '86889:1',\n",
       " '88280:1',\n",
       " '88283:1',\n",
       " '100288:1',\n",
       " '100300:1',\n",
       " '102447:1',\n",
       " '109932:1',\n",
       " '111823:1\\n']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(v.split(\"\\t\"))[-2].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id        F1        F2  Target\n",
      "0   0  0.197551  0.987172      -1\n",
      "1   1  0.979598  0.122393       1\n",
      "2   2  0.367976  0.984271      -1\n",
      "3   3  0.282004  0.946666       1\n",
      "4   4  0.720250  0.589025      -1\n",
      "5   5  0.077527  0.516489       1\n",
      "6   6  0.405469  0.982702      -1\n",
      "7   7  0.595486  0.850159      -1\n",
      "8   8  0.436349  0.987760       1\n",
      "9   9  0.319877  0.691997      -1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = np.arange(10)\n",
    "df['F1'] = np.random.rand(10,)\n",
    "df['F2'] = np.random.rand(10,)\n",
    "df['Target'] = list(map(lambda x: -1 if x < 0.5 else 1, np.random.rand(10,)))\n",
    "\n",
    "# X = df[np.setdiff1d(df.columns,['Id','Target'])]\n",
    "X = df[[\"Id\", \"F1\", \"F2\"]]\n",
    "y = df.Target\n",
    "\n",
    "print(df)\n",
    "\n",
    "dump_svmlight_file(X,y,'smvlight.dat',zero_based=True,multilabel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.123643</td>\n",
       "      <td>0.064664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.591428</td>\n",
       "      <td>0.267432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.987804</td>\n",
       "      <td>0.781589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599837</td>\n",
       "      <td>0.204119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.408970</td>\n",
       "      <td>0.306321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.673347</td>\n",
       "      <td>0.087296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.883486</td>\n",
       "      <td>0.607485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.077832</td>\n",
       "      <td>0.507009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.439109</td>\n",
       "      <td>0.884698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.888054</td>\n",
       "      <td>0.032524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F1        F2\n",
       "0  0.123643  0.064664\n",
       "1  0.591428  0.267432\n",
       "2  0.987804  0.781589\n",
       "3  0.599837  0.204119\n",
       "4  0.408970  0.306321\n",
       "5  0.673347  0.087296\n",
       "6  0.883486  0.607485\n",
       "7  0.077832  0.507009\n",
       "8  0.439109  0.884698\n",
       "9  0.888054  0.032524"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Path(\"../data/criteo/output_test/model.fm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/criteo/output_test')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not a.parent.exists():\n",
    "    a.parent.mkdir()\n",
    "if not a.exists():\n",
    "    a.touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pd.DataFrame([[1, 2, 3], [1, 2, 3], [1, 2, 3]], columns=[\"a\", \"b\", \"c\"], index=[\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Users\\hao.guo\\deepctr\\pywfm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywfm import FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with a.open(mode=\"r\") as f:\n",
    "    for v in f:\n",
    "        if v.startswith(\"a\"):\n",
    "            print(v)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WindowsPath' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-7ac80cab9385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'WindowsPath' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "a.read().decode(\"utf-8\").splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv(path, nr_thread, has_header):\n",
    "    # 计算每个子文件的数据行数\n",
    "    def calc_nr_lines_per_thread():\n",
    "        # 调用linux的wc命令获取文件行数\n",
    "        nr_lines = int(list(subprocess.Popen('sudo wc -l {0}'.format(path), shell=True, stdout=subprocess.PIPE).stdout)[0].split()[0])\n",
    "        if not has_header:\n",
    "            nr_lines += 1 # wc命令会自动跳过第一行不算\n",
    "        return nr_lines/nr_thread\n",
    "\n",
    "    # 打开子文件，并写入header\n",
    "    def open_with_header_written(path, idx, header):\n",
    "        f = open(path+'.__tmp__.{0}'.format(idx), 'w')\n",
    "        if not has_header:\n",
    "            return f\n",
    "        f.write(header)\n",
    "        return f\n",
    "\n",
    "    # 打开文件且跳过header\n",
    "    def open_with_first_line_skipped(path, skip=True):\n",
    "        f = open(path)\n",
    "        if not skip:\n",
    "            return f\n",
    "        next(f)\n",
    "        return f\n",
    "\n",
    "    header = open(path).readline()\n",
    "\n",
    "    nr_lines_per_thread = calc_nr_lines_per_thread()\n",
    "\n",
    "    # 遍历源文件写入各子文件\n",
    "    idx = 0\n",
    "    f = open_with_header_written(path, idx, header)\n",
    "    for i, line in enumerate(open_with_first_line_skipped(path, has_header), start=1):\n",
    "        f.write(line)\n",
    "        if i % nr_lines_per_thread == 0:\n",
    "            if idx < nr_thread - 1:\n",
    "                f.close()\n",
    "                idx += 1\n",
    "                f = open_with_header_written(path, idx, header)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones(shape=(50, 16))\n",
    "w = tf.convert_to_tensor(w)\n",
    "a = tf.expand_dims(a, axis=2)\n",
    "a = tf.tile(a, [1, 1, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.cast(w, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.tensordot(a, w, axes=([1], [0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 16, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9444389791664403"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0.05 / (1 - 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr2sparse(arr):\n",
    "    arr_tensor = tf.constant(arr)\n",
    "    arr_idx = tf.where(tf.not_equal(arr_tensor, 0))\n",
    "    arr_sparse = tf.SparseTensor(arr_idx, tf.gather_nd(arr_tensor, arr_idx), arr_tensor.shape)\n",
    "    return arr_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.random.randint(50, size=(50, 6))\n",
    "# ids = tf.convert_to_tensor(ids)\n",
    "# ids = tf.SparseTensor(ids)\n",
    "ids = arr2sparse(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.nn.embedding_lookup_sparse(\n",
    "    a, ids, sp_weights=None, combiner=\"sum\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.random.random(size=(50, 8))\n",
    "nums = tf.constant(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.random.random(size=(8, 4))\n",
    "embeds = tf.constant(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 8, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = tf.expand_dims(nums, axis=2)\n",
    "nums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nums * embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 8, 4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
